import base64
import binascii  # For base64.binascii.Error
import io
import logging
import os
import re
from collections.abc import AsyncIterator
from contextlib import asynccontextmanager

import requests
import torch
import uvicorn
from dotenv import load_dotenv
from fastapi import FastAPI, HTTPException
from PIL import Image
from pydantic import BaseModel, Field  # Moved to top with other third-party imports
from transformers import AutoModelForCausalLM, AutoProcessor

# Load environment variables from .env file
load_dotenv()

# Configuration
HOST = os.getenv("HOST", "0.0.0.0")
PORT = int(os.getenv("PORT", 8000))
LOG_LEVEL = os.getenv("LOG_LEVEL", "info").lower()
ROOT_PATH = os.getenv("ROOT_PATH", "")

# Logging setup
logging.basicConfig(level=LOG_LEVEL.upper(), format="%(asctime)s - %(name)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)

# --- Global variables for Model and Processor ---
model = None
processor = None
device = "cuda" if torch.cuda.is_available() else "cpu"


@asynccontextmanager
async def lifespan(app: FastAPI) -> AsyncIterator[None]:
    # Load the ML model
    logger.info("Application startup via lifespan...")
    global model, processor
    model_path = "sbintuitions/sarashina2-vision-8b"
    logger.info(f"Loading Sarashina2-Vision model and processor from {model_path} onto {device}...")
    try:
        processor = AutoProcessor.from_pretrained(model_path, trust_remote_code=True)
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            device_map=device,
            torch_dtype="auto",  # Let transformers handle dtype based on device/model
            trust_remote_code=True,
        )
        logger.info("Sarashina2-Vision model and processor loaded successfully.")
    except Exception as e:
        logger.error(f"Error loading Sarashina2-Vision model/processor: {e}", exc_info=True)
        # Optionally re-raise or handle as critical startup failure
    yield
    # Clean up the ML models and release the resources
    logger.info("Application shutdown via lifespan...")
    # Add any cleanup logic here if needed in the future


app = FastAPI(title="Sarashina2-Vision API", version="0.1.0", root_path=ROOT_PATH, lifespan=lifespan)

# --- Pydantic Models for Sarashina API ---


class SarashinaGenerateRequest(BaseModel):
    prompt: str = Field(..., description="User's text prompt for the model.")
    image: str | None = Field(default=None, description="URL or Base64 encoded image data.")
    max_new_tokens: int | None = Field(default=128, ge=1, description="Maximum number of new tokens to generate.")
    temperature: float | None = Field(default=0.0, ge=0.0, le=2.0, description="Sampling temperature.")


class SarashinaGenerateResponse(BaseModel):
    generated_text: str = Field(..., description="The text generated by the model.")


# Sarashina-specific endpoints will be added here
@app.post(
    "/generate",
    response_model=SarashinaGenerateResponse,
    summary="Generate text based on prompt and optional image.",
)
async def generate_handler(request_data: SarashinaGenerateRequest) -> SarashinaGenerateResponse:
    logger.info(f"Received /generate request: {request_data.model_dump_json(indent=2)}")

    if model is None or processor is None:
        logger.error("Model or processor not loaded. Cannot process request.")
        raise HTTPException(status_code=503, detail="Model is not ready, please try again later.")

    try:
        # Prepare text prompt
        chat_message = [{"role": "user", "content": request_data.prompt}]
        text_prompt_for_model = processor.apply_chat_template(
            chat_message,
            add_generation_prompt=True,
            tokenize=False,  # Returns a string
        )
        logger.debug(f"Applied chat template, text_prompt_for_model: {text_prompt_for_model!r}")

        images_for_processor: list[Image.Image] | None = None

        if request_data.image:
            image_input = request_data.image
            if image_input.startswith("http://") or image_input.startswith("https://"):
                logger.info(f"Fetching image from URL: {image_input}")
                try:
                    response = requests.get(image_input, stream=True, timeout=10)
                    response.raise_for_status()
                    pil_image = Image.open(io.BytesIO(response.content)).convert("RGB")
                    images_for_processor = [pil_image]
                    logger.info("Image fetched and converted to RGB successfully.")
                except requests.exceptions.RequestException as e:
                    logger.error(f"Failed to fetch image from URL '{image_input}': {e}")
                    raise HTTPException(status_code=400, detail=f"Failed to fetch image from URL: {str(e)}") from e
                except Exception as e:
                    logger.error(f"Invalid image at URL '{image_input}': {e}")
                    raise HTTPException(status_code=400, detail=f"Invalid image at URL: {str(e)}") from e
            else:
                logger.info("Processing Base64 encoded image.")
                try:
                    base64_data_str = image_input
                    if "," in base64_data_str:  # Handle data URI prefix
                        base64_data_str = base64_data_str.split(",", 1)[1]

                    if not re.fullmatch(r"[A-Za-z0-9+/]*={0,2}", base64_data_str):
                        logger.warning("Base64 string contains invalid characters.")
                        raise HTTPException(status_code=400, detail="Base64 string contains invalid characters.")

                    image_bytes = base64.b64decode(base64_data_str.encode("ascii"))
                    pil_image = Image.open(io.BytesIO(image_bytes)).convert("RGB")
                    images_for_processor = [pil_image]
                    logger.info("Base64 image decoded and converted to RGB successfully.")
                except binascii.Error as e:
                    logger.error(f"Invalid base64 encoding: {e}")
                    raise HTTPException(status_code=400, detail=f"Invalid base64 encoding: {str(e)}") from e
                except HTTPException:  # Re-raise if it's an HTTPException we threw
                    raise
                except Exception as e:
                    logger.error(f"Error processing base64 image: {e}", exc_info=True)
                    raise HTTPException(status_code=500, detail=f"Error processing base64 image: {str(e)}") from e

        logger.debug(
            f"Preparing inputs for processor. Text: {text_prompt_for_model!r}, "
            f"Images: {len(images_for_processor) if images_for_processor is not None else 0} image(s)"
        )
        inputs = processor(
            text=[text_prompt_for_model],
            images=images_for_processor,
            padding=True,
            return_tensors="pt",
        ).to(device)
        logger.debug("Inputs prepared and moved to device.")

        temp_for_generation = request_data.temperature
        if temp_for_generation is None:
            # Pydantic default is 0.0 if field omitted; if null, it's None here.
            # Model.generate usually defaults temperature to 1.0 if not provided or if None implies default.
            # Let's use 0.0 to be consistent with Pydantic's omission default.
            temp_for_generation = 0.0

        generate_kwargs = {
            "max_new_tokens": request_data.max_new_tokens,
            "temperature": temp_for_generation,
            "do_sample": temp_for_generation > 0.0,
        }
        if hasattr(processor, "tokenizer") and processor.tokenizer is not None:
            if processor.tokenizer.pad_token_id is not None:
                generate_kwargs["pad_token_id"] = processor.tokenizer.pad_token_id
            if processor.tokenizer.eos_token_id is not None:
                generate_kwargs["eos_token_id"] = processor.tokenizer.eos_token_id

        # Add stopping criteria
        # The Sarashina2 chat template uses "\n###" as a turn separator.
        # Using this as a stopping criterion can help prevent the model from generating subsequent turns.
        try:
            # Ensure get_stopping_criteria is available and the argument is a list of strings
            stop_strings = ["\n###"]
            stopping_criteria_instance = processor.get_stopping_criteria(stop_strings)
            if stopping_criteria_instance:  # It might return None or an empty list if not applicable
                generate_kwargs["stopping_criteria"] = stopping_criteria_instance
                logger.info(f"Added stopping_criteria for: {stop_strings}")
            else:
                logger.warning("Could not get stopping_criteria from processor or it was empty.")
        except AttributeError:
            logger.warning("processor.get_stopping_criteria is not available for this model/processor version.")
        except Exception as e:
            logger.warning(f"Error setting stopping_criteria: {e}")

        logger.info(f"Starting model generation with args: {generate_kwargs}")

        with torch.no_grad():
            output_ids = model.generate(**inputs, **generate_kwargs)
        logger.info("Model generation completed.")

        # Process output IDs to get only generated tokens, aligning with model card
        # The model card uses a loop for potential batch processing:
        # generated_ids = [ output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, output_ids) ]
        # Our API currently processes one request at a time, so the batch size is 1.

        generated_ids_for_batch_decode = []
        # Loop through the batch (even if it's just one item for this API)
        for i in range(inputs.input_ids.shape[0]):
            input_len = inputs.input_ids[i].shape[0]
            # Ensure output_ids[i] is long enough to contain new tokens
            if output_ids[i].shape[0] > input_len:
                generated_tokens = output_ids[i][input_len:]
            else:  # Output is not longer than input (e.g., pad/eos only, or max_length)
                generated_tokens = torch.tensor(
                    [], dtype=torch.long, device=output_ids.device
                )  # Empty tensor for no new tokens
            generated_ids_for_batch_decode.append(generated_tokens)

        output_text_list = processor.batch_decode(
            generated_ids_for_batch_decode, skip_special_tokens=True, clean_up_tokenization_spaces=True
        )

        # Since our API handles one request at a time, we expect one item in the list.
        output_text = output_text_list[0] if output_text_list else ""

        logger.info(f"Generated text: {output_text!r}")
        return SarashinaGenerateResponse(generated_text=output_text)

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error during generation: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"An internal error occurred: {str(e)}") from e


if __name__ == "__main__":
    logger.info(f"Starting Sarashina2-Vision API server on http://{HOST}:{PORT}{ROOT_PATH}")
    uvicorn.run("server:app", host=HOST, port=PORT, log_level=LOG_LEVEL, reload=True)
